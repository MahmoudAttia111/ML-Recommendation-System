# -*- coding: utf-8 -*-
"""recommend system me.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cOkCRvEBF0qPQbibwlP4zMfrWweEJOJt

## If you don't have nltk installed, run the following command:
"""

# !pip install nltk

"""# import libraries"""

import pandas as pd
import numpy as np
import scipy
import random
import math
import nltk
import sklearn

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize

"""## Download stopwords"""

nltk.download('stopwords')

"""## import dataset"""

article_df=pd.read_csv(fr"D:\data science\Machine Learning\New folder\recommand system\end-to-end-recommender-system-main\end-to-end-recommender-system-main\archive\shared_articles.csv")
interaction_df=pd.read_csv(fr"D:\data science\Machine Learning\New folder\recommand system\end-to-end-recommender-system-main\end-to-end-recommender-system-main\archive\users_interactions.csv")

article_df.head()

article_df['eventType'].unique()

"""## Filter articles"""

article_df=article_df[article_df['eventType']=='CONTENT SHARED']

article_df.shape

interaction_df['eventType'].unique()

"""## Feature engineering

### filling the users-tims matrix
"""

eventType_strength = {'VIEW':1.0, 'FOLLOW':3.0, 'BOOKMARK':2.5, 'LIKE':2.0, 'COMMENT CREATED':4.0}
interaction_df['eventType_strength'] = interaction_df['eventType'].map(eventType_strength)

interaction_df['eventType_strength']

"""## Keep only users with at least 5 interactions"""

users_interactions_count_df = interaction_df.groupby(['personId', 'contentId']).size().groupby('personId').size()
users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId']]
interactions_df = interaction_df.merge(users_with_enough_interactions_df, how='right', on='personId')
print(f"users with as least 5 interac {len(users_with_enough_interactions_df)}")

print(f"of interaction from users with at leaast 5 inter {len(interactions_df)}")

interactions_df.head(5)

interactions_df.groupby(['eventType','eventType_strength']).size()

"""## Apply log transformation to smooth the distribution"""

import math
def smooth_user_perferece(x):
    return math.log(1+x,2)
interactionFullDf=interactions_df.groupby(['personId','contentId'])['eventType_strength'].sum().apply(smooth_user_perferece).reset_index()
print(f"# of unique user'item interaction{len(interactionFullDf)}")
interactionFullDf.head(10)

"""## Clean text data"""

def clean_text(text):
    try:
        return text.lower().replace("\n", " ").replace("\"", "").strip()
    except:
        return ""

article_df['title'] = article_df['title'].apply(clean_text)
article_df['text'] = article_df['text'].apply(clean_text)

"""## item profiles"""

stopwords_list = stopwords.words('english') + stopwords.words('portuguese')
vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0.005, max_df=0.6,
                              max_features=6000, stop_words=stopwords_list)
tfidf_matrix = vectorizer.fit_transform(article_df['title'] + " " + article_df['text'])
item_ids = article_df['contentId'].tolist()

tfidf_matrix.shape

"""## Build train and test sets"""

interactions_train_df, interactions_test_df = train_test_split(interactionFullDf, stratify=interactionFullDf['personId'], test_size=0.3, random_state=42)

interactions_train_df[interactions_train_df['personId']==3609194402293569455].groupby('personId').size()

interactions_test_df[interactions_test_df['personId']==3609194402293569455].groupby('personId').size()

"""## Build user profiles"""

def get_item_profile(item_id):
    idx = item_ids.index(item_id)
    return tfidf_matrix[idx:idx + 1]

def get_item_profiles(ids):
    return scipy.sparse.vstack([get_item_profile(x) for x in ids])

def build_users_profile(person_id, interactions_indexed_df):
    interactions_person_df = interactions_indexed_df.loc[person_id]
    user_item_profiles = get_item_profiles(interactions_person_df['contentId'])
    user_item_strengths = np.array(interactions_person_df['eventType_strength']).reshape(-1, 1)
    weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) / np.sum(user_item_strengths)
    return normalize(np.asarray(weighted_avg))

def build_users_profiles():
    interactions_indexed_df = interactions_train_df[interactions_train_df['contentId'].isin(article_df['contentId'])].set_index('personId')
    return {person_id: build_users_profile(person_id, interactions_indexed_df) for person_id in interactions_indexed_df.index.unique()}

user_profiles=build_users_profiles()
len(user_profiles)

list(user_profiles.keys())[:5]

# show an example for user profiles >> random user
tfidf_feature_names = vectorizer.get_feature_names_out()
myprofile = user_profiles[ 2416280733544962613]
print(myprofile.shape)
pd.DataFrame(sorted(zip(tfidf_feature_names,user_profiles[ 2416280733544962613].flatten().tolist()),key = lambda x : -x[1])[:20],
            columns = ['token','relevance'])

"""## Content-based Filtering Model"""

class ContentBasedRecommender:
    ModelName = 'Content_Based'

    def __init__(self, item_df=None):
        self.item_ids = item_ids
        self.item_df = item_df

    def get_model_name(self):
        return self.ModelName

    def _get_similar_items_to_user_profile(self, person_id, topn=500):
        cosine_similarities = cosine_similarity(user_profiles[person_id], tfidf_matrix)
        similar_indices = cosine_similarities.argsort().flatten()[-topn:]
        similar_items = sorted([(item_ids[i], cosine_similarities[0, i]) for i in similar_indices], key=lambda x: -x[1])
        return similar_items

    def _recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):
        similar_items = self._get_similar_items_to_user_profile(user_id)
        filtered_items = list(filter(lambda x: x[0] not in items_to_ignore, similar_items))
        recommendations_df = pd.DataFrame(filtered_items, columns=['contentId', 'recStrength']).head(topn)
        if verbose:
            if self.item_df is None:
                raise Exception('item_df is required in verbose mode')
            recommendations_df = recommendations_df.merge(self.item_df, how='left', on='contentId')[['recStrength', 'contentId', 'title', 'url', 'lang']]
        return recommendations_df

content_model = ContentBasedRecommender(article_df)

"""## model evalutor"""

# to speed up the searches during evaluation
interactions_full_indexed_df = interactionFullDf.set_index('personId')
interactions_train_indexed_df = interactions_train_df.set_index('personId')
interactions_test_indexed_df = interactions_test_df.set_index('personId')

# top_n accuracy metrics consts
EVAl_RandomSample_NON_Interacted_ITEMS = 100
class ModelEvaluator:
    def get_items_interacted(self, person_id, interaction_df):
        interacted_items = interaction_df.loc[person_id]['contentId']
        return set(interacted_items if isinstance(interacted_items, pd.Series) else [interacted_items])

    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):
        interacted_items = self.get_items_interacted(person_id, interactions_full_indexed_df)
        all_items = set(article_df['contentId'])
        non_interacted_items = all_items - interacted_items
        sample_size = min(len(non_interacted_items), sample_size)  # ✅ إصلاح هنا
        random.seed(seed)
        return set(random.sample(list(non_interacted_items), sample_size))

    def _verify_hit_top_n(self, item_id, recommended_items, topn):
        try:
            index = next(i for i, c in enumerate(recommended_items) if c == item_id)
        except:
            index = -1
        hit = int(index in range(topn))
        return hit, index

    def evaluate_model_for_user(self, model, person_id):
        test_interactions = interactions_test_indexed_df.loc[person_id]
        if isinstance(test_interactions, pd.Series):
            interacted_items = {test_interactions['contentId']}
        else:
            interacted_items = set(test_interactions['contentId'])

        recommended_df = model._recommend_items(person_id, items_to_ignore=self.get_items_interacted(person_id, interactions_train_indexed_df), topn=500)
        valid_recommendations = []
        hits_at_5 = hits_at_10 = 0

        for item_id in interacted_items:
            non_interacted_sample = self.get_not_interacted_items_sample(person_id, 100, seed=item_id % (2 ** 32))
            test_items = non_interacted_sample.union({item_id})
            recs = recommended_df[recommended_df['contentId'].isin(test_items)]['contentId'].values

            hit5, _ = self._verify_hit_top_n(item_id, recs, 5)
            hit10, _ = self._verify_hit_top_n(item_id, recs, 10)

            hits_at_5 += hit5
            hits_at_10 += hit10

        return {
            'hits@5_count': hits_at_5,
            'hits@10_count': hits_at_10,
            'interacted_count': len(interacted_items)
        }

    def evaluate_model(self, model):
        people_metrics = []
        for idx, person_id in enumerate(interactions_test_indexed_df.index.unique().values):
            metrics = self.evaluate_model_for_user(model, person_id)
            metrics['_person_id'] = person_id
            people_metrics.append(metrics)
        detailed_df = pd.DataFrame(people_metrics)

        recall_at_5 = detailed_df['hits@5_count'].sum() / detailed_df['interacted_count'].sum()
        recall_at_10 = detailed_df['hits@10_count'].sum() / detailed_df['interacted_count'].sum()

        return {
            'modelName': model.get_model_name(),
            'recall@5': recall_at_5,
            'recall@10': recall_at_10
        }, detailed_df

"""## Prepare for evaluation"""

interactions_full_indexed_df = interactionFullDf.set_index('personId')
interactions_train_indexed_df = interactions_train_df.set_index('personId')
interactions_test_indexed_df = interactions_test_df.set_index('personId')

evaluator = ModelEvaluator()
cb_metrics, cb_details = evaluator.evaluate_model(content_model)
print("Global Metrics:\n", cb_metrics)
cb_details.head(10)

evaluator.get_items_interacted( 1874422396201148365,interactions_test_indexed_df)

import pickle
pickle.dump(item_ids, open(r"D:\data science\project kaggle\item_ids.p", "wb"))
pickle.dump(user_profiles, open(r"D:\data science\project kaggle\user_profiles.p", "wb"))
pickle.dump(tfidf_matrix, open(r"D:\data science\project kaggle\tfidf_matrix.p", "wb"))
pickle.dump(content_model, open(r"D:\data science\project kaggle\recommend_model.p", "wb"))

